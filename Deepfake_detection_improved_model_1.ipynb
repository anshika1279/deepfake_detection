{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGQyougLeB1e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Dense,BatchNormalization, Flatten, MaxPool2D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from keras.layers import Conv2D, Reshape\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from keras.backend import epsilon\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# The ImageDataGenerator class has been moved to tensorflow.keras.preprocessing.image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMC1aJWXhEPq",
    "outputId": "1430381b-85b6-40cf-893e-2a5e718b8692"
   },
   "outputs": [],
   "source": [
    "!file \"/content/archive.zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UDZrORvQi478",
    "outputId": "8c7f9fd3-54c0-443d-a6eb-1bb340139b1d"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"/content/archive.zip\"\n",
    "extract_path = \"/content/deepfake-detection\"\n",
    "\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(f\"Extraction completed! Files are in: {extract_path}\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(\"Error: The ZIP file is corrupted or invalid.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mGAiXShawiit",
    "outputId": "4fd34e16-8244-489f-b652-407bedb12a02"
   },
   "outputs": [],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538,
     "referenced_widgets": [
      "bcc81a2ee5e74cbb90641cfbbec39868",
      "462c1852f96b463e84a458d9f6425eb7",
      "29bf27614d1d4f7d9c42bae4830d957e",
      "7432e68dded6479889dee239ac2ace6d",
      "6bfb3f15e04641ff86bad08ee93408e5",
      "d69cb07c92ec4cd58d59426ba49020b0",
      "2155bb88c20f42798126a1f82b623071",
      "d865ddba0e394b06a4e2b8f90efd1bd4",
      "66beb831f9f74de2ba88dd23c6c444bd",
      "593a16f124ec4fe3bc037318359e979f",
      "33ee8db343cc45b2a593ebdc158385ec"
     ]
    },
    "id": "ZyD7vBAVwi9g",
    "outputId": "3dd658fc-2c50-4854-91af-f0ab6ffb65db"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import timm  # For loading Vision Transformer models\n",
    "import os\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------------------------\n",
    "# Define Deepfake Detection Model (ViT-based)\n",
    "# ------------------------------\n",
    "\n",
    "class DeepfakeDetectorViT(nn.Module):\n",
    "    def __init__(self, pretrained_model='vit_base_patch16_224'):\n",
    "        super(DeepfakeDetectorViT, self).__init__()\n",
    "\n",
    "        # Load pre-trained ViT model\n",
    "        self.feature_extractor = timm.create_model(pretrained_model, pretrained=True)\n",
    "\n",
    "        # Modify the classifier head for binary classification\n",
    "        num_ftrs = self.feature_extractor.head.in_features\n",
    "        self.feature_extractor.head = nn.Sequential(\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(num_ftrs, 1),  # Output for real/fake\n",
    "            nn.Sigmoid()  # Apply sigmoid for probability\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.feature_extractor(x)\n",
    "\n",
    "# ------------------------------\n",
    "# Initialize model, optimizer, and loss function\n",
    "# ------------------------------\n",
    "model = DeepfakeDetectorViT().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# ------------------------------\n",
    "# Load Deepfake dataset (same as before)\n",
    "# ------------------------------\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_set = datasets.ImageFolder(root='./deepfake-detection', transform=data_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)  # Adjust batch size if needed\n",
    "\n",
    "# ------------------------------\n",
    "# Training the Model (same as before)\n",
    "# ------------------------------\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True):\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1) # Convert labels to float and add dimension\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# ------------------------------\n",
    "# Evaluate the Model (same as before)\n",
    "# ------------------------------\n",
    "def evaluate_model(model, train_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.squeeze(1) == labels.float()).sum().item() # Compare after squeezing predicted and converting labels\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Deepfake Detection Accuracy: {accuracy:.2f}%\")\n",
    "# ... (same as in previous example) ...\n",
    "\n",
    "evaluate_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Su0WNlxhtHtD",
    "outputId": "6a940a07-e685-4b53-c706-a152f5e31021"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------------------------\n",
    "# Ensemble Model: MobileNetV2 + EfficientNet-B0 + Vision Transformer (ViT_B_16)\n",
    "# ------------------------------\n",
    "class DeepfakeEnsemble(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepfakeEnsemble, self).__init__()\n",
    "\n",
    "        # ---- MobileNetV2 ----\n",
    "        self.mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "        self.mobilenet.classifier = nn.Identity()\n",
    "\n",
    "        # ---- EfficientNet-B0 ----\n",
    "        self.efficientnet = models.efficientnet_b0(pretrained=True)\n",
    "        self.efficientnet.classifier = nn.Identity()\n",
    "\n",
    "        # ---- Vision Transformer (ViT_B_16) ----\n",
    "        # Using pretrained weights for better performance\n",
    "        self.vit = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT)\n",
    "        self.vit.heads = nn.Identity()   # Remove classification head\n",
    "\n",
    "        # ---- Combined Classifier ----\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        # MobileNetV2 → 1280, EfficientNet-B0 → 1280, ViT-B_16 → 768\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1280 + 1280 + 768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.mobilenet(x)\n",
    "        x2 = self.efficientnet(x)\n",
    "        x3 = self.vit(x)\n",
    "        x_concat = torch.cat((x1, x2, x3), dim=1)\n",
    "        return self.classifier(self.dropout(x_concat))\n",
    "# ------------------------------\n",
    "# Data Transform\n",
    "# ------------------------------\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # For ViT, EfficientNet, and MobileNet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# Dataset & Loader\n",
    "train_set = datasets.ImageFolder(root='./deepfake-detection', transform=data_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "\n",
    "# ------------------------------\n",
    "# Initialize model, loss, optimizer\n",
    "# ------------------------------\n",
    "model = DeepfakeEnsemble().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ------------------------------\n",
    "# Checkpoint paths\n",
    "# ------------------------------\n",
    "checkpoint_path = \"deepfake_ensemble_checkpoint.pth\"\n",
    "start_epoch = 0\n",
    "num_epochs = 5\n",
    "\n",
    "# Resume if checkpoint exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Train function with metrics tracking\n",
    "# ------------------------------\n",
    "def train_model(model, loader, criterion, optimizer, start_epoch, num_epochs):\n",
    "    history = {\n",
    "        'epoch': [],\n",
    "        'loss': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'auc': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        y_true, y_pred, y_scores = [], [], []\n",
    "\n",
    "        for inputs, labels in tqdm(loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            preds = (outputs > 0.5).float()\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_scores.extend(outputs.detach().cpu().numpy())\n",
    "\n",
    "        # Compute metrics\n",
    "        epoch_loss = running_loss / len(loader)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred)\n",
    "        rec = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Acc: {acc:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "        # Save metrics\n",
    "        history['epoch'].append(epoch + 1)\n",
    "        history['loss'].append(epoch_loss)\n",
    "        history['accuracy'].append(acc)\n",
    "        history['precision'].append(prec)\n",
    "        history['recall'].append(rec)\n",
    "        history['f1'].append(f1)\n",
    "        history['auc'].append(auc)\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    return history\n",
    "\n",
    "# ------------------------------\n",
    "# Training and History Tracking\n",
    "# ------------------------------\n",
    "history = train_model(model, train_loader, criterion, optimizer, start_epoch, num_epochs)\n",
    "\n",
    "# ------------------------------\n",
    "# Plotting Training Metrics\n",
    "# ------------------------------\n",
    "def plot_training_history(history):\n",
    "    epochs = history['epoch']\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(epochs, history['loss'], 'r-o')\n",
    "    plt.title(\"Loss vs Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(epochs, history['accuracy'], 'b-o')\n",
    "    plt.title(\"Accuracy vs Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(epochs, history['f1'], 'g-o')\n",
    "    plt.title(\"F1 Score vs Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(epochs, history['auc'], 'm-o')\n",
    "    plt.title(\"AUC vs Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(history['loss'], history['accuracy'], 'c-o')\n",
    "    plt.title(\"Accuracy vs Loss\")\n",
    "    plt.xlabel(\"Loss\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)\n",
    "\n",
    "# ------------------------------\n",
    "# Final Evaluation\n",
    "# ------------------------------\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_scores = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            outputs = model(inputs).squeeze(1)\n",
    "            preds = (outputs > 0.5).float()\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_scores.extend(outputs.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall   : {rec:.4f}\")\n",
    "    print(f\"F1 Score : {f1:.4f}\")\n",
    "    print(f\"ROC AUC  : {auc:.4f}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Confusion Matrix\n",
    "    # ------------------------------\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Fake\", \"Real\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # ------------------------------\n",
    "    # ROC Curve\n",
    "    # ------------------------------\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "Soggq6Eiv9KK",
    "outputId": "9f83da05-6504-4bab-9570-fc2b6b6b402f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "\n",
    "cm = np.array([[499, 0],\n",
    "               [8, 492]])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Fake\", \"Real\"])\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "plt.title(\"Confusion Matrix (Accuracy=0.9912, F1=0.9916)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CxcIYo35wDy1",
    "outputId": "99188c6b-0abf-416b-a5eb-2f91594ee19f"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_random_images(model, dataset, num_images=8):\n",
    "    model.eval()\n",
    "    indices = random.sample(range(len(dataset)), num_images)\n",
    "\n",
    "    y_true, y_pred, y_scores = [], [], []\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i, idx in enumerate(indices):\n",
    "        image, label = dataset[idx]\n",
    "        input_tensor = image.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor).item()\n",
    "            pred_label = 1 if output > 0.5 else 0\n",
    "            y_true.append(label)\n",
    "            y_pred.append(pred_label)\n",
    "            y_scores.append(output)\n",
    "\n",
    "        # Convert tensor to displayable image\n",
    "        img = image.permute(1, 2, 0).cpu().numpy()\n",
    "        img = np.clip(img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]), 0, 1)\n",
    "\n",
    "        plt.subplot(2, (num_images + 1)//2, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"True: {'Real' if label==1 else 'Fake'}\\nPred: {'Real' if pred_label==1 else 'Fake'}\\nConf: {output:.2f}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ------------------------------\n",
    "    # Compute and Display Metrics\n",
    "    # ------------------------------\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n--- Performance on Random Test Samples ---\")\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall   : {rec:.4f}\")\n",
    "    print(f\"F1 Score : {f1:.4f}\")\n",
    "    print(f\"ROC AUC  : {auc:.4f}\")\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Fake\", \"Real\"])\n",
    "    disp.plot(cmap=plt.cm.Purples)\n",
    "    plt.title(\"Confusion Matrix (Random Samples)\")\n",
    "    plt.show()\n",
    "test_random_images(model, train_set, num_images=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xy16DjzxtLu2"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HbrJs4QgpKPY",
    "outputId": "2fef6997-d967-41b2-ef27-4decbc1196b0"
   },
   "outputs": [],
   "source": [
    "!file \"/content/test_samples.zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y6JT0hinpSOc",
    "outputId": "b53d298c-10ca-4360-d2a0-a8a1394b163d"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"/content/test_samples.zip\"\n",
    "extract_path = \"/content/test_samples\"\n",
    "\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(f\"Extraction completed! Files are in: {extract_path}\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(\"Error: The ZIP file is corrupted or invalid.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZFvOEwyyvTm"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# --------------------------------------------\n",
    "# Test function for external random images folder\n",
    "# --------------------------------------------\n",
    "def test_model_on_folder(model, folder_path):\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_scores = [], [], []\n",
    "    image_paths, class_names = [], []\n",
    "\n",
    "    # Define transform (same as training)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Assuming folder structure:\n",
    "    # random_test_images/\n",
    "    # ├── Fake/\n",
    "    # └── Real/\n",
    "    #\n",
    "    # This mirrors the structure used by ImageFolder.\n",
    "    class_folders = sorted(os.listdir(folder_path))\n",
    "    print(f\"Detected classes: {class_folders}\")\n",
    "\n",
    "    for class_idx, cls in enumerate(class_folders):\n",
    "        class_path = os.path.join(folder_path, cls)\n",
    "        for img_path in glob.glob(os.path.join(class_path, \"*\")):\n",
    "            try:\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = model(image_tensor).item()\n",
    "                    pred_label = 1 if output > 0.5 else 0\n",
    "\n",
    "                y_true.append(class_idx)  # 0 = Fake, 1 = Real\n",
    "                y_pred.append(pred_label)\n",
    "                y_scores.append(output)\n",
    "                image_paths.append(img_path)\n",
    "                class_names.append(cls)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {img_path}: {e}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Compute metrics\n",
    "    # ------------------------------\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(\"\\n--- Performance on Random Test Folder ---\")\n",
    "    print(f\"Images Tested: {len(y_true)}\")\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall   : {rec:.4f}\")\n",
    "    print(f\"F1 Score : {f1:.4f}\")\n",
    "    print(f\"ROC AUC  : {auc:.4f}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Plot Confusion Matrix\n",
    "    # ------------------------------\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Fake\", \"Real\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix (Random Image Folder)\")\n",
    "    plt.show()\n",
    "\n",
    "    # ------------------------------\n",
    "    # Show few predictions\n",
    "    # ------------------------------\n",
    "    sample_indices = np.random.choice(len(y_true), min(8, len(y_true)), replace=False)\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        img = Image.open(image_paths[idx]).convert(\"RGB\")\n",
    "        plt.subplot(2, 4, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"True: {class_names[idx]}\\nPred: {'Real' if y_pred[idx]==1 else 'Fake'}\\nConf: {y_scores[idx]:.2f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 963
    },
    "id": "gOiOEX9qz2Sk",
    "outputId": "e9b6ea1f-05b4-4dd6-b693-96c5f110b5fc"
   },
   "outputs": [],
   "source": [
    "test_model_on_folder(model, \"./test_samples/test_samples\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPvnzWyNspmp"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nbformat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "nb_path = \"Deepfake_detection_improved_model.ipynb\"\n",
    "nb = nbformat.read(nb_path, as_version=4)\n",
    "\n",
    "# Remove all widget metadata\n",
    "for cell in nb.cells:\n",
    "    if \"metadata\" in cell and \"widgets\" in cell[\"metadata\"]:\n",
    "        cell[\"metadata\"].pop(\"widgets\")\n",
    "\n",
    "nbformat.write(nb, nb_path)\n",
    "print(\"Widget metadata removed!\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2155bb88c20f42798126a1f82b623071": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "29bf27614d1d4f7d9c42bae4830d957e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d865ddba0e394b06a4e2b8f90efd1bd4",
      "max": 346284714,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_66beb831f9f74de2ba88dd23c6c444bd",
      "value": 346284714
     }
    },
    "33ee8db343cc45b2a593ebdc158385ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "462c1852f96b463e84a458d9f6425eb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d69cb07c92ec4cd58d59426ba49020b0",
      "placeholder": "​",
      "style": "IPY_MODEL_2155bb88c20f42798126a1f82b623071",
      "value": "model.safetensors: 100%"
     }
    },
    "593a16f124ec4fe3bc037318359e979f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66beb831f9f74de2ba88dd23c6c444bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6bfb3f15e04641ff86bad08ee93408e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7432e68dded6479889dee239ac2ace6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_593a16f124ec4fe3bc037318359e979f",
      "placeholder": "​",
      "style": "IPY_MODEL_33ee8db343cc45b2a593ebdc158385ec",
      "value": " 346M/346M [00:03&lt;00:00, 191MB/s]"
     }
    },
    "bcc81a2ee5e74cbb90641cfbbec39868": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_462c1852f96b463e84a458d9f6425eb7",
       "IPY_MODEL_29bf27614d1d4f7d9c42bae4830d957e",
       "IPY_MODEL_7432e68dded6479889dee239ac2ace6d"
      ],
      "layout": "IPY_MODEL_6bfb3f15e04641ff86bad08ee93408e5"
     }
    },
    "d69cb07c92ec4cd58d59426ba49020b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d865ddba0e394b06a4e2b8f90efd1bd4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
